{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23d9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetching S&P 500 tickers from Wikipedia using BeautifulSoup...\n",
      "INFO:root:Wikipedia table columns: ['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry', 'Headquarters Location', 'Date added', 'CIK', 'Founded']\n",
      "INFO:root:Found 503 symbols\n",
      "INFO:root:Fetched 503 rows of market data\n",
      "INFO:root:CSV written: ./data\\sp500_raw_20251114_002312.csv\n",
      "INFO:root:Transformed shape: (503, 12) columns: ['DATE', 'SYMBOL', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'ADJ_CLOSE', 'VOLUME', 'CLOSE_CHANGE', 'CLOSE_PCT_CHANGE', 'DAILY_RANGE', 'DAILY_RANGE_PCT']\n",
      "INFO:root:CSV written: ./data\\sp500_transformed_20251114_002312.csv\n",
      "ERROR:root:Pipeline failed: Failed to upload ./data\\sp500_transformed_20251114_002312.csv to YOUR_S3_BUCKET/your/prefix/transformed/sp500_transformed_20251114_002315.csv: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "def _now_ts() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _write_csv(df: pd.DataFrame, prefix: str) -> str:\n",
    "    \"\"\"Write DataFrame to a local CSV and return the full path.\"\"\"\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    filename = f\"{prefix}_{_now_ts()}.csv\"\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    logging.info(\"CSV written: %s\", path)\n",
    "    return path\n",
    "\n",
    "def fetch_yfinance_data():\n",
    "    \"\"\"Pull current-day data for every S&P 500 ticker using BeautifulSoup to bypass JS issues.\"\"\"\n",
    "    logging.info(\"Fetching S&P 500 tickers from Wikipedia using BeautifulSoup...\")\n",
    "\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        raise RuntimeError(f\"Failed to fetch Wikipedia: {e}\")\n",
    "\n",
    "    # Use BeautifulSoup to find the correct table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find(\"table\", {\"id\": \"constituents\"})  # This is the correct table ID\n",
    "\n",
    "    if not table:\n",
    "        raise ValueError(\"Could not find S&P 500 constituents table on Wikipedia.\")\n",
    "\n",
    "    # Use pandas to read the table from the parsed HTML\n",
    "    df = pd.read_html(StringIO(str(table)))[0]\n",
    "\n",
    "    logging.info(\"Wikipedia table columns: %s\", df.columns.tolist())\n",
    "\n",
    "    # Normalize column name\n",
    "    symbol_col = None\n",
    "    for col in [\"Symbol\", \"Ticker\", \"symbol\", \"ticker\"]:\n",
    "        if col in df.columns:\n",
    "            symbol_col = col\n",
    "            break\n",
    "\n",
    "    if symbol_col is None:\n",
    "        raise KeyError(f\"Symbol column not found. Available: {df.columns.tolist()}\")\n",
    "\n",
    "    # Clean symbols: replace '.' with '-' (Yahoo Finance format)\n",
    "    symbols = df[symbol_col].astype(str).str.replace(\".\", \"-\", regex=False).tolist()\n",
    "    logging.info(\"Found %d symbols\", len(symbols))\n",
    "\n",
    "    rows = []\n",
    "    for sym in symbols:\n",
    "        try:\n",
    "            ticker = yf.Ticker(sym)\n",
    "            hist = ticker.history(period=\"1d\", auto_adjust=False)\n",
    "            if not hist.empty:\n",
    "                r = hist.iloc[-1]\n",
    "                rows.append({\n",
    "                    \"Datetime\": r.name.tz_localize(None).strftime(\"%Y-%m-%d\"),\n",
    "                    \"Symbol\": sym,\n",
    "                    \"Open\": round(r[\"Open\"], 6),\n",
    "                    \"High\": round(r[\"High\"], 6),\n",
    "                    \"Low\": round(r[\"Low\"], 6),\n",
    "                    \"Close\": round(r[\"Close\"], 6),\n",
    "                    \"Adj Close\": round(r.get(\"Adj Close\", r[\"Close\"]), 6),\n",
    "                    \"Volume\": int(r[\"Volume\"]),\n",
    "                })\n",
    "        except Exception as exc:\n",
    "            logging.warning(\"Failed for %s: %s\", sym, exc)\n",
    "\n",
    "    df_data = pd.DataFrame(rows)\n",
    "    logging.info(\"Fetched %d rows of market data\", len(df_data))\n",
    "\n",
    "    raw_path = _write_csv(df_data, \"sp500_raw\")\n",
    "    return raw_path\n",
    "\n",
    "def transform_data(raw_path):\n",
    "    \"\"\"Rename, calculate derived columns and write the final CSV.\"\"\"\n",
    "    if not raw_path or not os.path.exists(raw_path):\n",
    "        raise FileNotFoundError(f\"Raw CSV missing: {raw_path}\")\n",
    "    \n",
    "    df = pd.read_csv(raw_path)\n",
    "    df = df.rename(columns={\"Datetime\": \"Date\"})\n",
    "    df = df.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "    \n",
    "    df[\"close_change\"] = df.groupby(\"Symbol\")[\"Close\"].diff().fillna(0.0)\n",
    "    df[\"close_pct_change\"] = df.groupby(\"Symbol\")[\"Close\"].pct_change().fillna(0.0) * 100\n",
    "    \n",
    "    numeric_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\",\n",
    "                    \"close_change\", \"close_pct_change\"]\n",
    "    for c in numeric_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    \n",
    "    df[\"DAILY_RANGE\"] = df[\"High\"] - df[\"Low\"]\n",
    "    df[\"DAILY_RANGE_PCT\"] = (df[\"DAILY_RANGE\"] / df[\"Close\"]) * 100\n",
    "    \n",
    "    final_cols = [\n",
    "        \"Date\", \"Symbol\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\",\n",
    "        \"close_change\", \"close_pct_change\", \"DAILY_RANGE\", \"DAILY_RANGE_PCT\"\n",
    "    ]\n",
    "    final_df = df[final_cols].copy()\n",
    "    \n",
    "    rename_map = {\n",
    "        \"Date\": \"DATE\",\n",
    "        \"Symbol\": \"SYMBOL\",\n",
    "        \"Open\": \"OPEN\",\n",
    "        \"High\": \"HIGH\",\n",
    "        \"Low\": \"LOW\",\n",
    "        \"Close\": \"CLOSE\",\n",
    "        \"Adj Close\": \"ADJ_CLOSE\",\n",
    "        \"Volume\": \"VOLUME\",\n",
    "        \"close_change\": \"CLOSE_CHANGE\",\n",
    "        \"close_pct_change\": \"CLOSE_PCT_CHANGE\",\n",
    "        \"DAILY_RANGE\": \"DAILY_RANGE\",\n",
    "        \"DAILY_RANGE_PCT\": \"DAILY_RANGE_PCT\",\n",
    "    }\n",
    "    final_df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    logging.info(\"Transformed shape: %s columns: %s\", final_df.shape, final_df.columns.tolist())\n",
    "    \n",
    "    transformed_path = _write_csv(final_df, \"sp500_transformed\")\n",
    "    return transformed_path\n",
    "\n",
    "def upload_transformed_to_s3(transformed_path):\n",
    "    \"\"\"Upload only the transformed CSV to S3.\"\"\"\n",
    "    if not transformed_path or not os.path.exists(transformed_path):\n",
    "        raise FileNotFoundError(f\"Transformed CSV missing: {transformed_path}\")\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    s3_key = f\"{S3_PREFIX}transformed/sp500_transformed_{_now_ts()}.csv\"\n",
    "    s3.upload_file(\n",
    "        Filename=transformed_path,\n",
    "        Bucket=S3_BUCKET,\n",
    "        Key=s3_key\n",
    "    )\n",
    "    \n",
    "    s3_url = f\"s3://{S3_BUCKET}/{s3_key}\"\n",
    "    logging.info(\"Uploaded transformed CSV â†’ %s\", s3_url)\n",
    "    return s3_url, s3_key\n",
    "\n",
    "def load_to_snowflake(s3_key):\n",
    "    \"\"\"Load the data from S3 to Snowflake using COPY INTO.\"\"\"\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=SNOWFLAKE_USER,\n",
    "        password=SNOWFLAKE_PASSWORD,\n",
    "        account=SNOWFLAKE_ACCOUNT,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA\n",
    "    )\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Assuming the table exists and matches the CSV structure.\n",
    "    # Also assuming a storage integration is set up in Snowflake for the S3 bucket.\n",
    "    query = f\"\"\"\n",
    "    COPY INTO {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}.{SNOWFLAKE_TABLE}\n",
    "    FROM 's3://{S3_BUCKET}/{s3_key}'\n",
    "    STORAGE_INTEGRATION = {SNOWFLAKE_STORAGE_INTEGRATION}\n",
    "    FILE_FORMAT = (TYPE = 'CSV' FIELD_DELIMITER = ',' SKIP_HEADER = 1 FIELD_OPTIONALLY_ENCLOSED_BY = '\"');\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        logging.info(\"Data loaded to Snowflake table: %s\", SNOWFLAKE_TABLE)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Failed to load to Snowflake: %s\", e)\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        raw_path = fetch_yfinance_data()\n",
    "        transformed_path = transform_data(raw_path)\n",
    "        s3_url, s3_key = upload_transformed_to_s3(transformed_path)\n",
    "        load_to_snowflake(s3_key)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Pipeline failed: %s\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf6971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
